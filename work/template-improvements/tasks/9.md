---
status: planned
depends_on:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  - 7
  - 8
files_modified:
  - work/template-improvements/tests/test-scenarios.md
  - work/template-improvements/tests/test-results.md
---

# Task 9: Integration Testing

## Description

Create test scenarios and verify all components work together correctly.

## What to do

1. Create `work/template-improvements/tests/` directory

2. Create `test-scenarios.md` with test cases:

### Test Scenario 1: Orchestrator Intent Classification
```markdown
## Test 1: Orchestrator Intent Classification

**Input variations:**
1. "Что такое async/await?" → Expected: trivial
2. "Добавь кнопку логина" → Expected: explicit
3. "Улучши производительность" → Expected: exploratory
4. "Сделай что-нибудь с авторизацией" → Expected: ambiguous

**Verification:**
- [ ] trivial requests get direct response (no skill loading)
- [ ] explicit requests trigger skill selection
- [ ] exploratory requests trigger context-capture
- [ ] ambiguous requests ask clarifying question
```

### Test Scenario 2: Session Start Hook
```markdown
## Test 2: Session Start Hook

**Setup:**
1. Create STATE.md with incomplete task
2. Run session-start.sh

**Expected:**
- Hook detects incomplete work
- Shows task summary
- Shows countdown
- Allows cancellation

**Edge cases:**
- [ ] No STATE.md → silent exit
- [ ] Empty STATE.md → silent exit
- [ ] Completed tasks only → silent exit
```

### Test Scenario 3: Context Monitor
```markdown
## Test 3: Context Monitor

**Scenarios:**
1. Load 3 small files (<100 lines) → Should be <30%
2. Load 5 medium files (100-500 lines) → Should trigger 50% warning
3. Load 10+ files → Should trigger 70% block

**Verification:**
- [ ] Warning shown at ~50%
- [ ] Block message at ~70%
- [ ] Override option works
- [ ] Subagent suggestion clear
```

### Test Scenario 4: Background Task Tracking
```markdown
## Test 4: Background Task Tracking

**Setup:**
1. Start subagent-driven-development with 3 tasks

**Expected:**
- background-tasks.json created
- Tasks added with status: pending
- Status updated to running
- Status updated to completed
- Wave tracking correct

**Verification:**
- [ ] JSON file valid
- [ ] All task transitions logged
- [ ] Timestamps accurate
```

### Test Scenario 5: Autowork Pipeline
```markdown
## Test 5: Full Autowork Pipeline

**Input:** "autowork: добавь простую форму контакта"

**Expected flow:**
1. Orchestrator classifies as explicit
2. Mini user-spec interview (2-3 questions)
3. Tech-spec generated
4. Tasks created
5. subagent-driven-development runs
6. UAT triggered
7. Verification runs
8. Commit offered

**Verification:**
- [ ] Each phase executes
- [ ] User can intervene
- [ ] Fallback to manual works
```

### Test Scenario 6: Recovery Hooks
```markdown
## Test 6: Recovery Hooks

**Trigger each hook:**
1. Edit failure → edit-error-recovery.sh
2. Bash timeout → bash-timeout-recovery.sh
3. Test failure → test-failure-trigger.sh

**Verification:**
- [ ] Hooks execute on trigger
- [ ] Messages display correctly
- [ ] Retry limits work
- [ ] systematic-debugging loads on test failure
```

### Test Scenario 7: Self-Completion
```markdown
## Test 7: Self-Completion

**Setup:**
Create 6 TodoWrite items

**Expected:**
- Items 1-5 completed automatically
- Iteration 6 triggers max_iterations
- Remaining items reported

**Verification:**
- [ ] Auto-continues without prompts
- [ ] Stops at max (5)
- [ ] <done> marker when complete
- [ ] <max_iterations> marker when limit hit
```

3. Run each test and document in `test-results.md`:
```markdown
# Test Results

## Date: [execution date]

| Test | Status | Notes |
|------|--------|-------|
| 1. Orchestrator | ✅/❌ | |
| 2. Session Hook | ✅/❌ | |
| 3. Context Monitor | ✅/❌ | |
| 4. Background Tasks | ✅/❌ | |
| 5. Autowork | ✅/❌ | |
| 6. Recovery Hooks | ✅/❌ | |
| 7. Self-Completion | ✅/❌ | |

## Issues Found

### Issue 1: [Title]
- **Severity:** Critical/High/Medium/Low
- **Description:**
- **Fix:**

## Fixes Applied

- [ ] Issue 1 fixed in [file]
```

## Acceptance Criteria

- [ ] test-scenarios.md created with all 7 scenarios
- [ ] Each scenario has clear setup, expected, verification
- [ ] Tests executed manually
- [ ] test-results.md documents all results
- [ ] Issues documented with severity
- [ ] Critical/High issues fixed
- [ ] Re-tests confirm fixes

## Context Files

**Required:**
- All files from tasks 1-8
- `.claude/skills/testing/SKILL.md` (testing patterns)

## Technical Details

**Files:**
- `work/template-improvements/tests/test-scenarios.md` - Create new
- `work/template-improvements/tests/test-results.md` - Create new

**Test execution:**
- Manual testing (no automated framework)
- Document actual behavior vs expected
- Screenshot/log evidence where helpful
